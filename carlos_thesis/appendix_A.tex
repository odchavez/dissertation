
%\appendix 

\chapter{Probability distributions}
\label{appendix_A}

Here we describe the parameterization used for some of the probability distributions referred throughout the text. Namely: Gamma, Inverse Gamma, Exponential, Laplace, univariate and multivariate Student T (with location-scale parameters), univariate and multivariate Gaussian.

\section{Normal}

A continuous random variable $X$ follows a normal distribution $N(\mu, \sigma^2)$ if its density is

$$ f_X(x) = \frac{1}{\sqrt{2\pi}\sigma^2} \exp\left\{-\frac{1}{2}\left( \frac{x - \mu}{2\sigma}\right)^2\right\}, \ \ \ x \in \mathbb{R}.$$

It follows that $E(X) = \mu$ and $Var(X) = \sigma^2$.

\section{Multivariate Normal}

A continuous random vector $\bfX\in \mathbb{R}^d$ follows a $d-$variate normal distribution $N(\bfmu, \bfSigma)$ if its density is

$$f_{\bfX}(\bfx) = (2\pi)^{-\frac{d}{2}}\det|\Sigma|^{-\frac{1}{2}}\exp\left\{ -\frac{1}{2}(\bfX - \bfmu)^{\top} \bfSigma^{-1}(\bfX - \bfmu) \right\}, \ \ \ \bfx \in \mathbb{R}^d,$$

\noindent for $\bfSigma$ positive definite. It follows that $E(\bfX) = \bfmu$ and $Var(\bfX) = \bfSigma^2$.

\section{Gamma}

A continuous random variable $X$ follows a Gamma distribution $\mbox{Gama}(a,b)$ if its density is

$$f_X(x)=\frac{b^a}{\Gamma(a)}x^{a-1}e^{-bx}, \ \ \ x>0.$$

It folows that $E(X)=\frac{a}{b}$ and $Var(X)=\frac{a}{b^2}$.

\section{Inverse Gamma}

A continuous random variable $X$ follows an Inverse Gamma distribution with parameters $a$ and $b$, i. e.,  $X\sim GamaInv(a,b)$ if the random variable $Y = 1/X$ follows $Gamma(a, b).$ Then, $X$ has density

$$f_X(x)=\frac{b^a}{\Gamma(a)}x^{-a-1}e^{-\frac{b}{x}}, \ \ \ x>0.$$

It follows that $E(X)=\frac{b}{a-1}$ and $Var(X)=\frac{b^2}{(a-1)^2(a-2)}$.

\section{Student-t}

A continuous random variable $X$ follows a Stident T distribution with $\nu$ degrees of freedom, location $\mu$ and scale $\sigma$, if its density is

$$f_X(x)=\frac{\Gamma{\left(\frac{\nu+1}{2}\right)}}{\sigma\sqrt{\nu\pi}\Gamma(\frac{\nu}{2})}\left[\left(\frac{x-\mu}{\sigma}\right)^2+\nu\right]^{-\frac{\nu+1}{2}}, \ \ \ x\in \mathbb{R}.$$

In this case, we denote $X\sim T(\nu, \mu,\sigma)$. Under this parameterization, we have $E(X)=\mu$ if $\nu>1$ and $Var(X)=\sigma^2\times \frac{\nu}{\nu-2}$ for $\nu>2$. For $\nu = 1$, the average of the Student-T is not defined and if $\nu\leq 2$, the same holds for the variance.

\section{Multivariate Student-t}

A continuous random vector $\bfX \in \mathbb{R}^d$ follows a multivariate Student-T distribution with $\nu$ degrees of freedom and parameters $\bfmu$ and $\bfSigma$ if its density is

$$f_{\bfX}(\bfx)\propto\left[d+(\bfx-\bfmu)'\bfSigma^{-1}(\bfx-\bfmu)\right]^{-\frac{n+d}{2}} \ \ \ \bfx\in \mathbb{R}^d.$$

In this case, we denote $\bfX\sim T_n(\bfmu,\bfSigma)$. Here $\bfmu$ is the location parameter and the positive definite matrix $\bfSigma$ is the scale matrix. In this parameterization, $E(\bfX)=\bfmu$ if $\nu>1$ and $Var(\bfX)=\bfSigma\times \frac{\nu}{\nu-2}$ for $\nu>2$. Similar to the unidimensional case, $for \nu=1$ the mean of the distribution is not defined and if $\nu \leq 2$, the same happens for $\Sigma$.

An important result states that if $\bfX\sim T_n(\bfmu, \bfSigma)$, then the marginals are Student are also Student-T: $X_i \sim T_n(\mu_i, \sigma^2_i)$ where $\mu_i$ is the $i$-th entry of the mean vector and $\sigma^2_i=\bfSigma_{i,i}$.


\section{Laplace}

A continuous random variable $X \in \mathbb{R}^+$ follows a Laplace distribution with parameter $\lambda$, location $\mu$ and scale $\sigma>0$ if its density is


$$ f_X(x) = \frac{\lambda}{2\sigma}\exp\left\{ -\frac{\lambda |y - \mu |}{\sigma} \right\}, \ \ \ x > 0.$$

In this case, we denote $X \sim Laplace(\lambda, \mu, \sigma).$ It follows that $X \sim Laplace(\lambda, \mu, \sigma) \Rightarrow E(X) = \mu$ and $Var(X) = 2\sigma^2.$


\section{Negative Binomial}

A disctrete random variable $X$ follows a Negative Binomial distribution with parameters $n\in \mathbb{R}^{+}$ e $p \in (0,1)$ if $X$ its probability mass function

$$p_X(x)=\frac{\Gamma(x+n)}{\Gamma(x+1)\Gamma(n)}(1-p)^n p^x, \ \ \ x=0, 1, 2, \ldots$$

In this case, we denote $X\sim \mbox{NegBin}(n, p)$.

Under such parameterization, $E(X)=\frac{n p}{1-p}$ and $Var(X)=\frac{n p}{(1-p)^2}$.

\section{Log Normal}

A continuous random variable $X$ follows a $\mbox{LogNormal}(\mu, \sigma)$ distribution if the random variable $Y:=\log X$ follows a normal distribution $N(\mu, \sigma^2)$. In this case, the density function of $X$ is

$$f_X(x)=\frac{1}{x\sqrt{2\pi\sigma^2}}\exp\left\{-\left(\frac{\log x- \mu}{\sigma}\right)^2\right\}, \ \ \ x>0.$$

It follows that $E(X)=e^{\mu+\frac{\sigma^2}{2}}$ and $Var(X)=(e^{\sigma^2}-1)e^{2\mu+\sigma^2}$.
