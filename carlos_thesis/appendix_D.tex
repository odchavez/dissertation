
%\appendix 

\chapter{Appendix for Chapter 4}
\label{appendix_D}

\section{Full Conditionals for the POE Model}
\label{sec:full_cond}
\vspace{0.3 cm}

Here we briefly describe how to sample from each one of the full conditionals. In this section, we use $\mathds{1}(\cdot)$ to denote either an indicator function or th support of a truncated probability distribution. We also define the sets $\mathcal{P}^+_g:=\{ 1\leq t \leq T: \ e_{sg} = 1\}$ and $\mathcal{P}^+_t:=\{ 1\leq g \leq G: \ e_{sg} = 1\}$. The sets $\mathcal{P}^0_g, \ \mathcal{P}^-_g, \ \mathcal{P}^0_t$ and $\mathcal{P}^-_t$ are defined analogously.\\

\textbf{Updating $e_{sg}$}

$$p(e_{sg} = x \mid y, else) \propto \begin{cases}
\frac{\pi^+_g}{k^+_g} \times \mathds{1}(\alpha_s + \mu_g < y_{sg} < \alpha_s + \mu_g + k^+_g), &\mbox{ if } x = 1,\\
\pi^0_g \times N(y_{sg}; \ \alpha_s + \mu_g, \ \sigma^2_g), &\mbox{ if } x = 0,\\
\frac{\pi^-_g}{k^-_g} \times \mathds{1}(\alpha_s + \mu_g - k^-_g < y_{sg} < \alpha_s + \mu_g), &\mbox{ if } x = -1.
\end{cases}$$\\

\textbf{Updating $k^+_g$ and $k^-_g$}

\begin{multline*}
(k^+_g \mid y, else) \sim InvGamma(\# \mathcal{P}^+_g + \alpha_{k^+}, \ \beta_{k^+})\\
\mathds{1}\left(k^+_g > \max \left(\max_{t \in \mathcal{P}^+_g}\{y_{sg} - \alpha_s -\mu_g\}, \ k_0 \sigma_g \right) \right).
\end{multline*}

\begin{multline*}
(k^-_g \mid y, else) \sim InvGamma(\#\mathcal{P}^-_g + \alpha_{k^-}, \ \beta_{k^-})\\
\mathds{1}\left(k^-_g > \max \left(\max_{t \in \mathcal{P}^-_g }\{\alpha_s +\mu_g - y_{sg}\}, \ k_0 \sigma_g \right) \right).
\end{multline*}

\textbf{Updating $\bfpi_{g}$}

$$(\bfpi_g \mid y, else) \sim Dirichlet \left( (\ \#\mathcal{P}^-_g + \alpha^-_{\pi}, \ \#\mathcal{P}^0_g + \alpha^0_{\pi}, \ \#\mathcal{P}^+_g + \alpha^+_{\pi}) \right).$$

\textbf{Updating $\sigma^2_{g}$}

\begin{multline*}
(\sigma^2_g \mid y, else) \sim InvGamma \left( \frac{\#\mathcal{P}^0_g }{2} + \gamma, \ \frac{1}{2} \sum_{t \in \mathcal{P}^0_g } (y_{sg} - \alpha_s - \mu_g)^2 + \lambda\right)\\
\mathds{1}\left( \sigma^2_g < \frac{\min \left( k^+_g, k^-_g\right)^2}{k^2_0} \right).
\end{multline*}

\textbf{Updating $\mu_g$}

$$(\mu_g \mid y, else) \sim N(a_g, b_g) \mathds{1}\left( \max\left(M^+_g, M^-_g\right) < \mu_g < \min\left(m^{+}_g, m^-_g \right)\right),$$ 

where 
\begin{align*}
M^+_g &= \max\{y_{sg} - \alpha_s; \ t \in \mathcal{P}^+_g\} - k^+_g; \\ 
M^-_g &= \max\{y_{sg} - \alpha_s; \ t \in \mathcal{P}^-_g\}; \\ 
m^+_g &= \min\{y_{sg} - \alpha_s; \ t \in \mathcal{P}^+_g\}; \\ 
m^+_g &= \min\{y_{sg} - \alpha_s; \ t \in \mathcal{P}^-_g\} + k^-_g;\\
b_g &= (\#\mathcal{P}^0_g \sigma^{-2}_g + \tau^{-1}_{\mu})^ {-1};\\
a_g &= b_g \times \left[ \sigma^{-2}_g \sum_{t \in \mathcal{P}^0_g}(y_{sg} -\alpha_s ) + \tau^{-1}_{\mu}\theta_{\mu}\right]. 
\end{align*}

\textbf{Updating $\alpha_s$}
\begin{multline*}
(\alpha_s \mid y, else) \sim N( a_t, b_t) \\
\mathds{1}\left( \max\left(M^+_t, M^-_t\right) < \alpha_s < \min\left(m^{+}_t, m^-_t \right)\right)\mathds{1}\left(\sum^T_{t=1} \alpha_s = 0\right).
\end{multline*}

where
\begin{align*}
M^+_t &= \max\{y_{sg} - \mu_g - k^+_g; \ g \in \mathcal{P}^+_t\}; \\ 
M^-_t &= \max\{y_{sg} - \mu_g; \ g \in \mathcal{P}^-_t\}; \\ 
m^+_t &= \min\{y_{sg} - \mu_g; \ g \in \mathcal{P}^+_t\}; \\ 
m^+_t &= \min\{y_{sg} - \mu_g + k^-_g; \ g \in \mathcal{P}^-_t\};\\
b_t &= \left( \sum_{ g \in \mathcal{P}^0_t} \sigma^{-2}_g + \tau^{-1}_{\alpha} \right)^ {-1};\\
a_t &= b_t \times \left[ \sum_{t \in \mathcal{P}^0_t} \left( \frac{y_{sg} -\alpha_s}{\sigma^{2}_g } \right) + \tau^{-1}_{\alpha} \mu_{\alpha}\right]. 
\end{align*}

\textbf{Updating $\theta_{\mu}$}

$$(\theta_{\mu} \mid y, else) \sim N \left( \left(m_{\mu}s^{-2}_{\mu} + \tau^{-1}_{\mu}\sum^G_{g=1} \mu_g\right)(s^{-2}_{\mu} + G\tau^{-1}_{\mu})^{-1}, \ (s^{-2}_{\mu} + G\tau^{-1}_{\mu})^{-1}\right)$$

\textbf{Updating $\tau_{\mu}$}

$$(\tau_{\mu} \mid y, else) \sim InvGamma \left( \frac{G}{2} + a_{\tau_{\mu}}, \ \frac{1}{2} \sum^G_{g=1}(\mu_g - \theta_{\mu})^2 + b_{\tau_{\mu}}\right)$$

\textbf{Updating $\beta_{k^+}$}

$$ (\beta_{k^+} \mid y, else) \sim Gamma\left(G \alpha_{k^+} + a_{\beta_{k^+}}, \ b_{\beta_{k^+}} + \sum^G_{g=1}\frac{1}{k^+_g}\right)$$

\textbf{Updating $\beta_{k^-}$}

$$ (\beta_{k^-} \mid y, else) \sim Gamma\left(G \alpha_{k^-} + a_{\beta_{k^-}}, \ b_{\beta_{k^-}} + \sum^G_{g=1}\frac{1}{k^-_g}\right)$$

\textbf{Updating $\alpha_{k^+}$}

$p(\alpha_{k^+} \mid y, else)$ is not analytically available since

$$p(\alpha_{k^+} \mid y, else) \propto \Gamma(\alpha_{k^+})^{-G} \left( \frac{\beta^G_{k^+}}{ \prod^G_{g=1}k^+_g}\right)^{\alpha_{k^+}}e^{-\alpha_{k^+}\lambda_{\alpha_{k^+}}}.$$

One way of (approximately) sampling from this distribution is through the Metropolis-Hastings scheme. 

We specify a proposal $q(\alpha^{new}_{k^+} \mid \alpha^{old}_{k^+})$ corrected by the acceptance probability $\alpha(\alpha^{new}_{k^+} \mid \alpha^{old}_{k^+}):=\min\left\{1, \  r(\alpha^{new}_{k^+} \mid \alpha^{old}_{k^+}) \right\},$ where

$$r(\alpha^{new}_{k^+} \mid \alpha^{old}_{k^+}) := \frac{q(\alpha^{old}_{k^+} \mid \alpha^{new}_{k^+})p(\alpha^{new}_{k^+} \mid y, else)}{q(\alpha^{new}_{k^+} \mid \alpha^{old}_{k^+})p(\alpha^{old}_{k^+} \mid y, else)}.$$

Our proposal is a random-walk on $\log \alpha_{k^+},$ i.e., $\log \alpha^{new}_{k^+} \sim N(\log\alpha^{old}_{k^+}, V^+)$ for some fixed $V^+>0$. which implies a LogNormal proposal density on the original scale with $q(\alpha^{new}_{k^+} \mid \alpha^{old}_{k^+}) = N(\alpha^{new}_{k^+}; \alpha^{old}_{k^+}, V^+)\times 1/\alpha^{new}_{k^+}$.

It is straightforward to verify that
\begin{align*}
\log r(\alpha^{new}_{k^+} \mid \alpha^{old}_{k^+}) = (\log \alpha^{old}_{k^+} &- \log \alpha^{new}_{k^+}) - G\left[ \log \Gamma(\alpha^{new}_{k^+}) - \log \Gamma(\alpha^{old}_{k^+}) \right] + \\
&+(\alpha^{new}_{k^+} - \alpha^{old}_{k^+}) \left[  G \log \beta_{k^+} - \sum^G_{g=1} \log k^+_g - \lambda_{\alpha_{k^+}}\right].
\end{align*}

\textbf{Updating $\alpha_{k^-}$}

Updating $\alpha_{k^-}$ is entirely analogous to updating $\alpha_{k^+}$.

\subsection{Sampling from truncated distributions within MCMC}
\label{sec:trunc_dists}

In this section we describe the Gibbs sampler augmentation scheme to asymptotically sample from the truncated inverse gamma and truncated normal distributions that appear in appendix \ref{sec:full_cond}. Although sampling algorithms for truncated distributions can be easy derived, sometimes even by the inverse c.d.f. method, the resulting algorithm can often be numerically unstable (take the truncated Gaussian distribution for example). In such cases, it could be advantageous to use an approximate sampler if it is more robust to computational errors. In this regard, we follow the directions on \cite{damien2001}. 

The univariate normal sampling scheme can be seen as a particular instance of the algorithm for multivariate normals or as an extension of the sampling scheme for univariate standard normals that are both described in \cite{damien2001}. The algorithm to sample from truncated inverse gamma is very similar to the one that samples from the truncated Gamma. We describe both sampling schemes here solely for the purpose of completeness.


\subsubsection{Truncated normal}

Suppose a truncated Gaussian distribution for the random variable $X$: $X\sim N(\mu, \sigma^2)\mathds{1}(a < X < b)$, i.e., $f_{X}(x) \propto \exp\left\{-\frac{(x - \mu)^2}{2 \sigma^2} \right\}\mathds{1}(a < x<b)$, where we could have $a=-\infty$ or $b=+\infty$ to represent unilateral truncation. We define the auxiliary variable $Y$ through the joint density

$$f_{X,Y}(x, y) \propto \mathds{1}(0<y<e^{-\frac{(x-\mu)^2}{2\sigma^2}})\mathds{1}(a<x<b)$$

\noindent so that the implied marginal for $X$ matches the original $N(\mu, \sigma^2)\mathds{1}(a < X < b)$. The full conditional distributions are 

\begin{align}
(Y\mid X=x) &\sim Unif\left(0, \ \exp\left\{-\frac{(x - \mu)^2}{2\sigma^2}\right\}\right),\label{eq:trunc_normal_1}\\
(X \mid Y=y) &\sim Unif\left(\max (a, \ \mu - \sqrt{-2\sigma^2\log y}), \ \ \min (b, \ \mu + \sqrt{-2\sigma^2\log y})\right).\label{eq:trunc_normal_2}
\end{align}

Within the MCMC scheme described in section \ref{sec:full_cond}, we include sampling from the auxiliary variables $Y_{\mu_g}$ and $Y_{\alpha_t}$ corresponding to the full conditional distributions of $\mu_g$, and $\alpha_t$ respectively. The auxiliary variables are sampled from \eqref{eq:trunc_normal_1} while the original variables are sampled from \eqref{eq:trunc_normal_2}, with the appropriate values of $\mu$, $\sigma^2$, $a$ and $b$.


\subsubsection{Truncated inverse gamma}

Suppose $X \sim InvGamma(\alpha, \beta)\mathds{1}(a<x<b)$, i.e., $f_{X}(x) \propto x^{-\alpha -1} e^{-\frac{x}{\beta}}\\ \mathds{1}(a < x < b).$ We define the joint density of $X$ and $Y$:

$$f_{X,Y}(x, y) \propto x^{-\alpha - 1} \mathds{1}(0<y<e^{-\frac{x}{\beta}})\mathds{1}(a<x<b)$$

\noindent so that the implied marginal for $X$ matches the original $InvGamma(\alpha, \beta)\mathds{1}(a<X<b)$. The full conditional distributions are 
\begin{align}
(Y \mid X=x) \sim Unif(0, e^{-\frac{x}{\beta}}),\label{eq:trunc_gamma_inv1} \\
f_{X\mid Y}(x \mid y) \propto x^{-\alpha -1} \mathds{1}(M(y)<x<b),
\label{eq:trunc_gamma_inv2}
\end{align}

\noindent where $M(y):=\max\left( a, \ -\frac{\beta}{\log y}\right)$. The inverse c.d.f. method provides an efficient way to sample from \eqref{eq:trunc_gamma_inv2}: sample $U \sim Unif(0,1)$ then evaluate the transformed variable 

$$\frac{M(y)}{ \left[U( \{M(y)/b\}^{\alpha} -1 ) + 1\right]^{\frac{1}{\alpha}}},$$

\noindent which  will be distributed as \eqref{eq:trunc_gamma_inv2}. 

Within the MCMC scheme described in section \ref{sec:full_cond}, we include sampling from the auxiliary variables $Y_{k^+_g}$, $Y_{k^-_g}$ and $Y_{\sigma^2_g}$ corresponding to the full conditional distributions of $k^+_g$, $k^-_g$ and $\sigma^2_g$ respectively. The auxiliary variables are sampled from \eqref{eq:trunc_gamma_inv2} while the original variables are sampled from \eqref{eq:trunc_gamma_inv1}, with the appropriate values of $\alpha$ and $\beta$.

\section{Full Conditionals for Matching Cell Line and Patients Model}
\label{sec:full_cond_2}
\vspace{0.3 cm}

This appendix describes steps of the Gibbs sampler algorithm used to carry out posterior inference.

Define $S^{x,k}_j:=\{i:\delta^{x,k}_i=j\}$ with $x$ being either $c$ or $p$. In the remainder of this appendix section, we will denote by $s_j$ the single element in the set $S^{c,k}_j$ (it could even be $s_j=\emptyset$), omitting the superscripts for simplicity.

The full posterior (up to a normalizing constant depending solely on the data $\bfd$) can be factorized as 

\begin{align}
p(\bfPsi \mid \bfd) &\propto \left[\prod^G_{g=1} p(\sigma^{-2}_g) p(\sigma^{-2}_{1g}) p(\sigma^{-2}_{2g})\right] p(\bfw \mid \pi_0, \alpha_0) \left[\prod^{K_{\bfw}}_{k=1} p(\bfdelta^{p,k})p(\bfdelta^{c,k}\mid \bfdelta^{p,k})\right]\times \nonumber\\
&\times\prod^{K_{\bfw}}_{k=1} \prod_{g:w_g=k}\prod^{J_k}_{j=1}p(\theta^*_{jg} \mid \mu_{0g}, \sigma^2_{0g})\times\left[\prod_{g=1}^G p(\mu_{0g})p(\mu_{1g})p(\mu_{2g})\right]\times\nonumber\\
&\times\prod^{K_{\bfw}}_{k=1} \prod_{g:w_g=k}\prod^{J_k}_{j=1}\left[ p(d^c_{ig}\mid \theta^*_{jg}, \sigma^2_g)^{\mathds{1}(S^{c,k}_j=\{i\}\neq \emptyset)}\prod_{i: \delta^{p,k}_i=j}p(d^p_{ig}\mid \theta^*_{jg}, \sigma^2_g)\right]\times\nonumber\\
&\times\prod^{K_{\bfw}}_{k=1} \prod_{g:w_g=k}\left[ \prod_{i: \delta^{c,k}_i=0}p(d^c_{ig}\mid \mu_{1g}, \sigma^2_g, \sigma^2_{1g})\prod_{i: \delta^{p,k}_i=0}p(d^p_{ig}\mid \mu_{1g}, \sigma^2_g, \sigma^2_{1g})\right]\times\nonumber\\
&\times\prod_{g:w_g=0}\left[\prod^{N^p}_{i=1} p(d^p_{ig} \mid \mu_{2g}, \sigma^2_{g}, \sigma^2_{2g}) \prod^{N^c}_{i=1} p(d^c_{ig} \mid \mu_{2g}, \sigma^2_{g}, \sigma^2_{2g})\right].
\label{eq:full_posterior}
\end{align}

\textbf{Updating $\theta^*_{jg}$:}

\begin{equation*}
p(\theta^*_{jg} \mid \bfd, \bfPsi_{-\theta^*_{jg}}) \propto N(\theta^*_{jg}; \ \mu_{0g}, \sigma^2_{0g})\prod_{i\in S^{c,w_g}_j}N(d^c_{ig};\ \theta^*_{jg}, \sigma^2_g) \prod_{i\in S^{c,w_g}_j} N(d^p_{ig}; \ \theta^*_{jg}, \sigma^2_g)
\end{equation*}
\begin{multline*}
(\theta^*_{jg} \mid \bfd, \bfPsi_{-\theta^*_{jg}}) \sim N\left( \frac{(\sum_{i \in S^{c,w_g}_j}d^c_{ig} + \sum_{i \in S^{p,w_g}_j}d^p_{ig})\sigma^{-2}_g + \mu_{0g}\sigma^{-2}_{0g}}{(|S^{c,w_g}_j| + |S^{p,w_g}_j|)\sigma^{-2}_g + \sigma^{-2}_{0g}}, \right. \\ 
\left.\frac{1}{(|S^{c,w_g}_j| + |S^{p,w_g}_j|)\sigma^{-2}_g + \sigma^{-2}_{0g}}\right).
\end{multline*}

\textbf{Updating $\bfdelta^{p,k}$:}


We follow \cite{bush1996} and sample the cluster membership indicators $\delta^{p,k}$ within a Gibbs block marginalizing $\bftheta^*$ out, i.e., by sampling $p(\delta^{p,k}_i=j \mid \bfd, \bfPsi_{-(\delta^{p,k}_i, \bftheta^*)})$ for $i=1, \ldots, N^p$. These updates together with the previous one where we sampled $\theta^*_{jg} \sim p(\theta^*_{jg} \mid \bfPsi_{-\theta^*_{jg}})$ for all $j$ and $g$, asymptotically provides a blocked joint sample from  $p(\bftheta^*, \bfdelta^{p,k} \mid \bfd, \bfPsi_{-(\bfdelta^{p,k}, \bftheta^*)})$.

Denote by $A^-_{p,k}$ the number of active patient samples within protein sample $k$ excluding patient $i$ and define $S^{p,k-}_j:=S^{p,k}_j\setminus\{i\}$. Then $\bfdelta^{p,k}\sim ZEPU(\alpha_{p,k}, \pi_{pk})$ implies

\begin{equation}
P(\delta^{p,k}_i = j \mid \bfdelta^{p,k}_{-i}) = 
\begin{cases}
\pi_{pk}, \ \ \ j=0\\
(1 - \pi_{pk})\frac{ |S^{p,k-}_j | }{ \alpha_{pk} + A^-_{p,k}}, \ \ \ j=1, \ldots, J^-_k \\
(1 - \pi_{pk})\frac{\alpha_{p,k}}{\alpha_{pk} + A^-_{p,k}}, \ \ \ j = J^-_k + 1.
\end{cases}
\label{eq:deltap_deltap_minus_i}
\end{equation}

Using equation \eqref{eq:deltap_deltap_minus_i}, we obtain 

\begin{align}
P(\delta^{p,k}_i = j \mid \bfdelta^{c,k}, \bfdelta^{p,k}_{-i})&\propto p(\bfdelta^{c,k} \mid \bfdelta^{p,k})P(\delta^{p,k}_i = j \mid \bfdelta^{p,k}_{-i}) \nonumber \\
&\propto P(\delta^{p,k}_i = j \mid \bfdelta^{p,k}_{-i})\sum^{ \min\{J_k, N^c\} }_{n=0} {J_k \choose n} {{N^c}\choose n} n!
\label{eq:blablabla}
\end{align}

\noindent analytically.  Notice that $J_k$ varies with $\delta^{p,k}_i$ so the summation term cannot by omitted from \eqref{eq:blablabla}.

After marginalizing $\theta^*_{jg}$ out from $p(d^p_{ig} \mid \bfd^p_{-i g}, d^c_{s_j g}, \delta^{p,k}_i=j, \bfPsi_{-\delta^{p,k}_i})$, we obtain

\begin{align*}
p&(d^p_{ig} \mid \bfd^p_{-i g}, d^c_{s_j g}, \delta^{p,k}_i=j, \bfPsi_{-(\delta^{p,k}_i, \bftheta^*)}) \\
&= \sqrt{\frac{(|S^{p,k-}_j|\sigma^{-2}_g + \sigma^{-2}_{0g})(|S^{p,k-}_j|\sigma^{-2}_g + \sigma^{-2}_{0g} + \sigma^{-2}_g)}{(2\pi\sigma^2_g)}}\times\\
&\times\exp\left\{ -\frac{1}{2} \left[ {d^{p}_{ig}}^2\sigma^{-2}_g + (|S^{p,k-}_j|\sigma^{-2}_g + \sigma^{-2}_{0g}) \vphantom{\sum_{\ell \in S^{p,k-}_j}}\times\\
&\hspace{1.5cm}\left. \left. \times\left(\sigma^{-2}_g\sum_{\ell \in S^{p,k-}_j}d^p_{\ell g} + d^{c}_{s_jg}\mathds{1}(S^{c,k}_j \neq \emptyset)\sigma^{-2}_g + \mu_{0g}\sigma^{-2}_{0g}\right)^2\right]\right\}\times  \\
&\times \exp\left\{\frac{1}{2}(\sigma^{-2}_g + \sigma^{-2}_{0g}+ |S^{p,k-}_j|\sigma^{-2}_g)^{-1} \vphantom{\sum_{\ell \in S^{p,k-}_j}} \right.\times\\
&\hspace{1.5cm}\times\left.\left[ d^p_{ig}\sigma^{-2}_g + \left( \sum_{\ell \in S^{p,k-}_j}d^p_{\ell g} + d^c_{s_jg} \mathds{1}(S^{c,k}_j\neq \emptyset)\right)\sigma^{-2}_g + \mu_{0g}\sigma^{-2}_{0,g}\right]\right\}.
\end{align*}


Using equation \eqref{eq:blablabla}, we get

\begin{align}
P&(\delta^{p,k}_i=j \mid \bfPsi_{-(\delta^{p,k}_i, \bftheta^*)})\propto \nonumber\\
&\propto
\begin{cases}
 \left[\prod_{g:w_g=k}N(d^p_{ig} \mid \mu_{1g}, \sigma^2_g + \sigma^2_{1g})\right] \pi_{pk}\sum^{J^-_k+1}_{n=0}\frac{1}{(N^c-n)!} b_j, &j=0 \vspace{0.5 cm}\nonumber\\
\prod_{g:w_g=k}p(d^p_{ig} \mid \bfd^p_{-i g}, d^c_{s_j g}, \delta^{p,k}_i=j, \bfPsi_{-(\delta^{p,k}_i, \bftheta^*)})\times\\ 
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \times(1 - \pi_{pk})\frac{|S^{p,k}_j - \{i\} |}{\alpha_{pk} + A^-_{p,k}}\sum^{J^-_k+1}_{n=0}\frac{1}{(N^c-n)!}b_j, &j=1, \ldots, J^-_k   \vspace{0.5 cm}\nonumber\\
\prod_{g:w_g=k}N(d^p_{ig} \mid \mu_{0g}, \sigma^2_g + \sigma^2_{0g}) \times\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \times(1 - \pi_{pk})\frac{\alpha_{p,k}}{\alpha_{pk} + A^-_{p,k}}\sum^{J^-_k+2}_{n=0}\frac{1}{(N^c-n)!}b_j, \ \ &j = J^-_k + 1,
\end{cases}
\end{align}

\noindent where $J^-_k$ is the number of active clusters of patients within protein group $k$ after removing patient $i$ and $b_j:=\mathds{1}(|S^{c,k}_j| = 1)\mathds{1}(|S^{p,k}_j| > 0) + \mathds{1}(|S^{c,k}_j| = 0)$ is a binary variable that enforces non-empty active clusters of cell lines to contain at least one patient sample as well.

%\begin{align*}
%p(\bfdelta^{c,k} &\mid \bfd, \bfPsi_{-\delta^{c,k}}) \propto \frac{\alpha^{n^{c,k}_0}}{ \prod^{N^c-n^{c,k}_0}_{l = 0}(J_k - l + \alpha) }\times \mathds{1}(\bfdelta^{c,k} \in \mathcal{B}^{c,k})\times \\
%&\times\left[ \prod_{i: \delta^{c,k}_i = 0}N(\theta^c_{ig}; \ \mu_{1g}, \sigma^2_{1g})N(d^c_{ig}; \ \theta^c_{ig}, \sigma^2_g)\right] \times \prod^{J_k}_{j=1}\prod_{i: \delta^{c,k}_i=j}N(d^c_{ig}; \ \theta^*_{jg}, \sigma^2_g)\times\\
%&\times \prod_{g:w_g=k}\prod^{J_k}_{j=1}N(\theta^*_{jg}; \ \mu_{0g}, \sigma^2_{0g}).
%\end{align*}


\textbf{Updating $\bfdelta^{c,k}_i$:}


\begin{align*}
p&(\delta^{c,k}_i = j \mid \bfd, \bfPsi_{-\delta^{c,k}_i}) \propto\\
&\propto 
\begin{cases}
\prod_{g:w_g = k} N(d^c_{ig}; \ \theta^*_{jg}, \sigma^2_{1g}), &j>0, \ S^{p,k}_j \neq \emptyset, \ S^{c,k}_j=\emptyset \\
\prod_{g:w_g = k}N(d^c_{ig}; \ \mu_1, \sigma^2_{g}+ \sigma^2_{1g}), &j=0\\
0, &\mbox{otherwise.}
\end{cases}
\end{align*}

We also define a Metropolis-Hastings algorithm to sample from $\bfdelta^{c,k}$ in a way that hopefully produces Markov Chains with better mixing properties. Here we omit the upper indexes $c,k$ from $\bfdelta^{c,k}$ for clarity of exposition.

Recall that the Metropolis-Hastings algorithm produces a new sample $\bfdelta^{t+1}$ from $\bfdelta^t$ according to an auxiliary transition probability $q(\bfdelta^{t+1} \mid \bfdelta^t)$ that is irreducible and aperiodic. Then $\bfdelta^{t+1}$ is accepted with probability $\alpha(\bfdelta^{t+1} \mid \bfdelta^{t}):= \max\{1, r(\bfdelta^{t+1} \mid \bfdelta^{t})\}$ where $r(\bfdelta^{t+1} \mid \bfdelta^{t}):=\frac{q(\bfdelta^{t}\mid\bfdelta^{t+1})p(\bfdelta^{t+1})}{q(\bfdelta^{t+1}\mid\bfdelta^{t})p(\bfdelta^{t})}$.

We define two types of transitions and at each iteration we uniformly chose one of them at random. 

\textbf{Type I:} We take an active cell line and switch it with one of the inactive cell lines. Under such proposal, \ $q(\bfdelta^{t+1}\mid \bfdelta^t)=\frac{1}{|S^{c,k}_0|(N^c - |S^{c,k}_0|)}.$ Under this proposal, the acceptance ratio reduces to

$$r(\bfdelta^{c,k}(t+1) \mid \bfdelta^{c,k}(t)) = \prod_{g: w_g = k} \frac{N(d^c_{i_0(t+1)}; \mu_1, \sigma^2_g + \sigma^2_{1g})N(d^c_{ig}; \theta^*_{jg}, \sigma^2_g)\big\lvert_{j=\delta^{c,k}_{i_1(t+1)}}}{N(d^c_{i_0(t)}; \mu_1, \sigma^2_g + \sigma^2_{1g})N(d^c_{ig}; \theta^*_{\ell g}, \sigma^2_g)\big\lvert_{\ell=\delta^{c,k}_{i_1(t)}}},$$

\noindent where $i_1(x)$ and $i_0(x)$ respectively denote the active and inactive cell lines selected by the proposal at time $x$ (before switching, when $x=t$; and after switching, when $x=t+1$).

\textbf{Type II:} Randomly pick an active cluster $j$. If $|S^{c,k}_j(t)|=0$ (no active cell line in cluster $j$), assign an inactive cell line to cluster $j$ uniformly at random. On the other hand, if $|S^{c,k}_j(t)|=1$ we reassign the only active cell line $i\in S^{c,k}_j(t)$ from cluster $j$ to the group of inactive cell lines by making $\delta^{c,k}_{i}(t+1)=0$. Under such proposal, we have $q(\bfdelta^{t+1}\mid \bfdelta^t) = \frac{1}{J_k |S^{c,k}_0(t)|}\mathds{1}(|S^{c,k}_j|=0)+\frac{1}{J_k} \mathds{1}(|S^{c,k}_j|=1)$. Under this proposal,

$$r(\bfdelta^{c,k}(t+1) \mid \bfdelta^{c,k}(t)) = 
\begin{cases}
\frac{N(d^c_{ig}; \ \theta^*_{jg}, \sigma^2_g)|S^{c,k}_0(t)|}{N(d^c_{ig}; \ \mu_1, \sigma^2_{0g}\sigma^2_g)}\Big\vert_{i \in S^{c,k}_j(t+1)}, &S^{c,k}_j(t) = \emptyset \vspace{0.3 cm}\\
\frac{N(d^c_{ig}; \ \mu_1, \sigma^2_{0g}\sigma^2_g)}{N(d^c_{ig}; \ \theta^*_{jg}, \sigma^2_g)|S^{c,k}_0(t+1)|}\Big\vert_{i \in S^{c,k}_j(t)}, &S^{c,k}_j(t) = \{i\} \neq \emptyset.
\end{cases}$$

