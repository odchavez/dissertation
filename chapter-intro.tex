\chapter{Introduction}

\section{Overview}

This thesis will focus on the relational learning in two areas: the modeling of text and user roles in networks, and the relative treatment of individuals as related to algorithmic fairness.

With the growth of social media, an increasing amount of new data generated occurs in a network context.  In Chapter \ref{ch:TopicBlockModel} I explore a model which combines the intuitive content description of topic modelling with the user archetyping of a stochastic blockmodel.  The combination allows us to describe contextual relationships of users (nodes) in the network and the content typical of the messages they exchange.

In Chapter \ref{ch:MonoFair} I explore the concept of monotonic individual fairness.  Individual fairness \citep{dwork2012fairness} formalizes the intuitive idea similar individuals should be treated similarly.  I extend this to the idea to formalize the idea that individuals' relative treatment should follow their relative qualification.  I accomplish this by identifying non-protected attributes which should have a monotonic relationship with the outcome and enforcing that the learned prediction function maintains this monotonicity.

I extend this concept in Chapter \ref{ch:SoftMonoFair} to the more realistic scenario in which monotonicity is derived from a sample of expert ratings.  This allows for more complex relationships between individuals; fairness might dictate that an increase in one attribute has a larger effect than a similar increase in another.  For example, a person a few violent felonies might considered more dangerous than a person with more non-violent misdemeanors.

\section{Background}

    \subsection{Bayesian Inference} \label{sec:intro_bayes}
    
        Bayesian statistics view the parameters of a statistical model as themselves being random variables based on an interpretation of probability as the uncertainty of belief about a system or outcome.  Mathematically, Bayesian statistics relies on Bayes' formula, derived from conditional probability as
        $$ \Pr(\theta | X) = \frac{\Pr(X | \theta) \Pr(\theta)}{\Pr(X)}. $$
        While classical (or frequentist) statistics focus exclusively on the \emph{likelihood} of the data ($\Pr(X | \theta)$ and what values of $\theta$ produce high likelihoods of the observed data, Bayesian statistics utlizes a \emph{posterior} (\emph{a posteriori}) distribution ($\Pr(\theta | X)$) of belief about $\theta$.  This posterior distribution is estimated via a \emph{prior} (\emph{a posteriori}) distribution ($\Pr(\theta)$, usually of belief about the distribution) of $\theta$ and, when a proportionate distribution is not sufficient, the \emph{evidence} ($Pr(X)$) of the data under the model.
        
        A variety of techniques have been developed for inference about parameters in Bayesian settings utilizing analytical, optimization, stochastic, and other approaches.  Chapter \ref{ch:TopicBlockModel} will utilize a stochastic method based on \emph{Monte Carlo sampling}.  Monte Carlo methods produce estimates about the posterior of a target parameter by producing samples from a distribution which approximates the posterior.  For high dimensional $\theta$, it is often intractible to produce samples of all parameters simultaneously, so sampling is accomplished by partitioning the parameter space and iteratively updating the partitions, e.g.
        $$ \Pr(\theta_1 | X, \theta_2) = \frac{\Pr(X | \theta_1, \theta_2) \Pr(\theta_1, \theta_2)}{\Pr(X, \theta_2)}. $$
        Since $\Pr(X, \theta_2)$ doesn't depend on $\theta_1$, it can be viewed as a normalizing constant in the posterior and is usually omitted, leading to a proportional expression,
        $$ \Pr(\theta_1 | X, \theta_2) \propto \Pr(X | \theta_1, \theta_2) \Pr(\theta_1, \theta_2).$$
        Depending on the exact setting, a variety of tools can be used to generate samples of $\theta_1$, including relatively old methods like the Metropolis\cite{metropolis1953equation} algorithm or Gibbs sampling\cite{geman1984stochastic} and relatively modern methods like Hamiltonian Monte Carlo\cite{girolami2011riemann} and the No-U-Turn Sampler (NUTS)\cite{hoffman2014no}.

    \subsection{Topic Models}  \label{sec:intro_topics}
    
        Topic models are a popular family of hierarchical Bayesian models for semantic analysis of corpora of documents.  The canonical model of this type is Latent Dirichlet Allocation \cite{blei2003LDA}, where each document is associated with a Dirichlet-distributed distribution over $T$ ``topics'', which themselves are Dirichlet-distributed distributions over words that tend to concentrate on semantically coherent topics. Each word  in the  document is assumed to have been generated by sampling a topic from that document's distribution over topics, and then sampling a word from the topic's distribution over words. 
        
        This basic model has been extended in a number of directions. A hierarchy of Dirichlet processes can be used to construct a Bayesian nonparametric variant with an unbounded number of topics \citep{Teh:2007}; a logistic normal distribution can be used to induce correlations between topics \citep{ctm}; time dependence has been incorporated to track topic evolution over time \citep{blei2006dynamic}.
    
    \subsection{Stochastic Blockmodels} \label{sec:intro_sbm}
    
    
        Stochastic blockmodels \citep{Wang:Wong:1987,Snijders:Nowicki:1997} are a popular class of generative models that assume that each node within a network is associated with one of $K$ latent clusters or communities. Each pair $(k,\ell)$ of communities is associated with a latent parameter $\lambda_{k,\ell}$, which parametrizes the interactions between members of those communities. Typically, the network is assumed to be binary, and the interactions are modeled as Bernoulli random variables.
        
        A number of variants to the basic stochastic blockmodel have been proposed. \citep{Karrer:Newman:2011} uses a gamma/Poisson link in place of a beta/Bernoulli, to obtain distributions over integer-valued networks, and also incorporates a per-node parameter that allows nodes in the same community to have different degree distribution. The Infinite Relational Model \citep{irm} allows a potentially infinite number of communities, with membership probabilities distributed according to a Dirichlet process. Rather than restrict each node to a single cluster, the Mixed Membership Stochastic Blockmodel \citep{Airoldi:Blei:Fienberg:Xing:2008} associates each node with a distribution over clusters, allowing nodes to perform several social roles.
    
    \subsection{Neural Networks}  \label{sec:intro_nns}
    
        Neural networks are a class of extremely functions defined by a series of alternating linear and non-linear transformations.  The theory that neural networks can act as universal function approximators goes back several decades\cite{cybenko1989approximation}, and since then their use has grown steadily.
        
        A single layer of a network can be expressed as
        $$ h_l = f_l( h_{l-1}; W_l, b_l, \sigma) = \sigma\left( W_l h_{l-1} + b_l \right) $$
        with $h_{l}$ being the output of the $l$'th layer, $W_l$ being a matrix of weights describing a linear transformation from $\mathbb{R}^{|h_{l-1}|}$ to $\mathbb{R}^{|h_l|}$, a bias vector $b_l$, and a non-linear transformation function on $\mathbb{R}^{|h_{l-1}|}$.  It commonly notated that $h_0$ is the input $x$ and $h_L$, i.e. the output of the final $L$'th layer, is the output $y$.
        
        There are several significant drawbacks of neural networks: the weight matrices $W_l$ incur an extremely large number of parameters, the combination of exchangeable weight matrices and non-linear activation function creates a highly-multimodal and non-convex parameter space, and the resulting functions are prone to overfitting sample data.
    
    \subsection{Fairness in Machine Learning}  \label{sec:intro_fairml}
    
        Machine learning algorithms trained to infer relationships, classify individuals or predict individuals' future performance tend to replicate biases inherent in the data \citep{Caliskan:Bryson:Narayanan:2017,Bornstein:2018,Angwin:Larson:Muttu:Kirchner:2016}. Worse, when these algorithms are used as tools in policy decision making, they can form parts of feedback loops that magnify discriminatory effects. For example, predictive policing algorithms aim to predict where crimes will take place, but are trained on data from where crimes are reported or arrests are made -- which can be skewed by biased policing and might not reflect the true crime map. If police officers are sent to areas with high predictive crime rate, they will tend to make more arrests there, increasing the algorithm's confidence and amplifying discrepancies between the crime rate and the arrest rate \citep{Ensign:2018,Lum:Isaac:2016}. 

    \subsubsection{Fairness Metrics}  \label{sec:intro_fairmetrics}
    
        Definitions of fairness in machine learning are generally (but not exclusively) divided into two camps based on their level of attention: group-level fairness and individual-level fairness.
        
        \emph{Individual fairness} aims to ensure that two individuals $u$ and $v$ with non-protected attributes $X_u, X_v$ have similar outcomes if $X_u$ and $X_v$ are similar, even if their protected attributes differ. Concretely, \citep{dwork2012fairness} describes a score function $f$ as individually fair if it is Lipschitz-continuous w.r.t.\ some metric $\mathcal{D}$ on $X$, i.e.\
        \begin{equation}d(f(X_u), f(X_v)) \le \mathcal{D}(X_u, X_v) ~\forall~ u, v \in \mathcal{U}\label{eqn:IndFair}\end{equation}
        where $d$ is a metric on the space of outcomes. This encapsulates the notion that if two individuals are similar in terms of non-protected attributes, they should have similar outcomes.
        
        Conversely, \emph{group fairness} metrics aim to minimize population-level imbalances. For example, the notion of demographic parity \citep{dwork2012fairness} requires that the predicted outcome $\hat{Y}$ is independent of the protected variable $A$. Equalized odds \citep{HarPriSre2016} requires that the predicted outcome $\hat{Y}$ is independent of $A$ conditioned on the true outcome $Y$, allowing our predictor to depend on $A$ via $Y$. Equalized opportunity \citep{HarPriSre2016} relaxes this condition in a classification task where the outcome $\hat{Y}=1$ is seen as more desirable than $\hat{Y}=0$, to require conditional independence between predictor $\hat{Y}$ and protected variable $\hat{A}$ only when $Y=1$. \cite{AgaBeyDudLanWal2018} show that demographic parity, equalized odds, and their variants can be expressed in terms of a set of linear constraints.  In many cases, individual notions of fairness are at odds with group notions of fairness. For example, \cite{dwork2012fairness} shows that individually fair functions achieve perfect demographic parity if and only if the distribution over individuals is similar across demographic groups.


