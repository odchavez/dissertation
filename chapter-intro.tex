\chapter{Introduction}

\section{Overview}

This thesis will focus on the relational learning in two areas: the modeling of text and user roles in networks, and the relative treatment of individuals as related to algorithmic fairness.

With the growth of social media, an increasing amount of new data generated occurs in a network context.  In Chapter \ref{ch:TopicBlockModel} I explore a model which combines the intuitive content description of topic modelling with the user archetyping of a stochastic blockmodel.  The combination allows us to describe contextual relationships of users (nodes) in the network and the content typical of the messages they exchange.

In Chapter \ref{ch:MonoFair} I explore the concept of monotonic individual fairness.  Individual fairness \citep{dwork2012fairness} formalizes the intuitive idea similar individuals should be treated similarly.  I extend this to the idea to formalize the idea that individuals' relative treatment should follow their relative qualification.  I accomplish this by identifying non-protected attributes which should have a monotonic relationship with the outcome and enforcing that the learned prediction function maintains this monotonicity.

I extend this concept in Chapter \ref{ch:SoftMonoFair} to the more realistic scenario in which monotonicity is derived from a sample of expert ratings.  This allows for more complex relationships between individuals; fairness might dictate that an increase in one attribute has a larger effect than a similar increase in another.  For example, a person a few violent felonies might considered more dangerous than a person with more non-violent misdemeanors.

\section{Background}

    \subsection{Bayesian Inference}
    
        TBD.  Include MCMC intro.
        
    \subsection{Topic Models}
    
        Topic models are a popular family of hierarchical Bayesian models for semantic analysis of corpora of documents.  The canonical model of this type is Latent Dirichlet Allocation \cite{blei2003LDA}, where each document is associated with a Dirichlet-distributed distribution over $T$ ``topics'', which themselves are Dirichlet-distributed distributions over words that tend to concentrate on semantically coherent topics. Each word  in the  document is assumed to have been generated by sampling a topic from that document's distribution over topics, and then sampling a word from the topic's distribution over words. 
        
        This basic model has been extended in a number of directions. A hierarchy of Dirichlet processes can be used to construct a Bayesian nonparametric variant with an unbounded number of topics \citep{Teh:2007}; a logistic normal distribution can be used to induce correlations between topics \citep{ctm}; time dependence has been incorporated to track topic evolution over time \citep{blei2006dynamic}.
    
    \subsection{Stochastic Blockmodels}
    
    
        Stochastic blockmodels \citep{Wang:Wong:1987,Snijders:Nowicki:1997} are a popular class of generative models that assume that each node within a network is associated with one of $K$ latent clusters or communities. Each pair $(k,\ell)$ of communities is associated with a latent parameter $\lambda_{k,\ell}$, which parametrizes the interactions between members of those communities. Typically, the network is assumed to be binary, and the interactions are modeled as Bernoulli random variables. In a Bayesian setting, we place conjugate (in the binary case, beta) priors on the $\lambda_{k,\ell}$, and a Dirichlet-multinomial prior on the community memberships.
    
        A number of variants to the basic stochastic blockmodel have been proposed. \citep{Karrer:Newman:2011} uses a gamma/Poisson link in place of a beta/Bernoulli, to obtain distributions over integer-valued networks, and also incorporates a per-node parameter that allows nodes in the same community to have different degree distribution. The Infinite Relational Model \citep{irm} allows a potentially infinite number of communities, with membership probabilities distributed according to a Dirichlet process. Rather than restrict each node to a single cluster, the Mixed Membership Stochastic Blockmodel \citep{Airoldi:Blei:Fienberg:Xing:2008} associates each node with a distribution over clusters, allowing nodes to perform several social roles.
    
    \subsection{Neural Networks}
    
        TBD
    
    \subsection{Fairness}
    
        Machine learning algorithms trained to infer relationships, classify individuals or predict individuals' future performance tend to replicate biases inherent in the data \citep{Caliskan:Bryson:Narayanan:2017,Bornstein:2018,Angwin:Larson:Muttu:Kirchner:2016}. Worse, when these algorithms are used as tools in policy decision making, they can form parts of feedback loops that magnify discriminatory effects. For example, predictive policing algorithms aim to predict where crimes will take place, but are trained on data from where crimes are reported or arrests are made -- which can be skewed by biased policing and might not reflect the true crime map. If police officers are sent to areas with high predictive crime rate, they will tend to make more arrests there, increasing the algorithm's confidence and amplifying discrepancies between the crime rate and the arrest rate \citep{Ensign:2018,Lum:Isaac:2016}. 

    
        \subsubsection{Metrics}
        
            Definitions of fairness in machine learning are generally (but not exclusively) divided into two camps based on their level of attention: group-level fairness and individual-level fairness.
            
            \textbf{Individual fairness} aims to ensure that two individuals $u$ and $v$ with non-protected attributes $X_u, X_v$ have similar outcomes if $X_u$ and $X_v$ are similar, even if their protected attributes differ. Concretely, \citep{dwork2012fairness} describes a score function $f$ as individually fair if it is Lipschitz-continuous w.r.t.\ some metric $\mathcal{D}$ on $X$, i.e.\
            \begin{equation}d(f(X_u), f(X_v)) \le \mathcal{D}(X_u, X_v) ~\forall~ u, v \in \mathcal{U}\label{eqn:IndFair}\end{equation}
            where $d$ is a metric on the space of outcomes. This encapsulates the notion that if two individuals are similar in terms of non-protected attributes, they should have similar outcomes. We can think of individual fairness as avoiding \textit{resentment} w.r.t.\ the protected variable: Under an individually fair algorithm, no-one would have achieved a better solution if they had a different protected variable. 
            
            
            Conversely, \textbf{group fairness} metrics aim to minimize population-level imbalances. For example, the notion of demographic parity \citep{dwork2012fairness} requires that the predicted outcome $\hat{Y}$ is independent of the protected variable $A$. Equalized odds \citep{HarPriSre2016} requires that the predicted outcome $\hat{Y}$ is independent of $A$ conditioned on the true outcome $Y$, allowing our predictor to depend on $A$ via $Y$. Equalized opportunity \citep{HarPriSre2016} relaxes this condition in a classification task where the outcome $\hat{Y}=1$ is seen as more desirable than $\hat{Y}=0$, to require conditional independence between predictor $\hat{Y}$ and protected variable $\hat{A}$ only when $Y=1$. \cite{AgaBeyDudLanWal2018} show that demographic parity, equalized odds, and their variants can be expressed in terms of a set of linear constraints.  In many cases, individual notions of fairness are at odds with group notions of fairness. For example, \cite{dwork2012fairness} shows that individually fair functions achieve perfect demographic parity if and only if the distribution over individuals is similar across demographic groups.

    
