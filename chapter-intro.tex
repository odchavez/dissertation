\chapter{Introduction}

\section{Overview}

This thesis will focus on relational learning in two areas: the modeling of text and user roles in networks, and the relative treatment of individuals as related to algorithmic fairness.  

The exponential growth of computers' influence in our lives has been driven not by individual computers but by their networks, and more so by the possibilities of the combinatorial interactions of the users of those networks.  Users increasingly base their lives on computer networks, from mundane economic activities to higher order socialization.  These networks affect ourselves, our internal perceptions, our families, our jobs, and politics at every level.

This wealth of data has created a wealth of open problems.  The first was ``big data,'' the notion of data sets too large for traditional computing resources and algorithms.  This has been answered with a tidal wave of hardware, especially distributed systems.  It has also been a boon to optimization research, with complexity calculations becoming a de facto requirement in the development of new methods.

Relational learning, which aims to treat entities as being mutually dependent and to perform inference about the dependencies, has always been in conflict with optimization.  The number of relationships to be learned grows quadratically with the number of entities in a network; a social network which doubles in size will quadruple the number of possible relationships.  The value of the data stored in the network, however, also rises as the depth of information about each individual user deepens and as we gain data to make shared inference about groups of users.

This growth of information is not without danger, however.  The increasing availability data has lead to its increasing misuse, either deliberately (e.g. fraud, surveillance, misinformation) or inadvertently.  Inadvertent misuse can create a variety of problems ranging from classical statistical errors like selection bias to more complex errors like model overfitting creating overconfidence and reliance.  This misuse can occur top-down by powerful actors but also happens, perhaps more commonly, by individual users who are not psychologically aware of the effect of the signals they consume and produce.



Chapter \ref{ch:TopicBlockModel} explores a model which combines the intuitive content description of topic modelling with the user archetyping of a stochastic blockmodel.  The combination allows us to describe contextual relationships of users (nodes) in the network and the content typical of the messages they exchange.

The crux of the model is the intuition that individuals in one homogeneous group should behave similarly when interacting with individuals of another homogeneous group.  This reduces the complexity of modeling in two ways.  First, rather than modeling $n$ individuals, we can model $k \ll n$ clusters of people, and rather than constructing $n^2$ models for interactions, we can model $k^2$ archetypal message patterns.  This greatly reduces the state space of models, which assists in fighting the enemy of $\mathcal{O}(n^2)$ complexity.  

Second, this model allows us to make inference about individuals' interactions which have not yet been observed.  For instance, when trying to make service recommendations to an individual who has not visited a store previously, it would be useful to know how that individual has interacted with other businesses, and how other individuals like this one has interacted the businesses at hand.



Chapter \ref{ch:MonoFair} explores the concept of monotonic individual fairness.  Individual fairness \citep{dwork2012fairness} formalizes the intuitive idea that similar individuals should be treated similarly, which this work extends to formalize the idea that individuals' relative treatment should follow their relative qualification.  Put simply, individuals will reject the fairness of a system if they can point to an individual who receives a more favorable outcome despite ``worse" attributes.  Even though such relative treatment may be reasonable within the context of historic discrimination and various concepts of group-level fairness, it is nonetheless controversial and generates opposition when it allows relative treatments that generate resentment.

This work proposes a system for enforcing that the learned prediction function maintains monotonicty w.r.t~ user-specific non-protected attributes.  In this context, we require a system which can learn arbitrary monotonic functions over large parameters spaces.  We review artificial neural networks with modified weight structure which can generate functions that are monotonic w.r.t.~ a subset of their inputs. We then show that such functions can balance multiple objectives by maintaining comparable accuracy and group fairness measures to non-monotonic neural networks and a related fair learning model.



Chapter \ref{ch:SoftMonoFair} extends this concept to the more flexible scenario in which monotonicity is derived from a sample of arbiter ratings. This is motivated by two problems: the inadequacy of monotonicity w.r.t.\ individual attributes to capture holistic value systems, and the difficulty of formalizing a priori knowledge about such holistic systems. Fairness might dictate that an increase in one attribute has a larger effect than a similar increase in another.  For example, a person who has committed a few violent felonies might considered more dangerous than a person who has committed a large number of non-violent misdemeanors.

The proposed model uses arbiter ratings, i.e.\ responses from a group of impartial individuals on a series of queries, to assess what the the relative treatment of individuals must be to be considered fair.  Because resentment often occurs at an individual level and without consideration for larger concepts of group fairness, we do not require that the arbiters are experts or versed in group fairness and historic effects that could create bias.

We then propose a system which can use conditional neural networks to simultaneously learn from the observed outcomes of historic data and the arbiter ratings obtained by survey.  This model is flexible enough to also simultaneously incorporate group fairness, although post hoc adjustment would have to balance the compromises between the accuracy, resentment, and group equality.  This model is also compatible with the individual attribute monotonicity of the previous chapter in the case that some a priori structure is specified.

We demonstrate this model in two scenarios.  The first is a synthetic example which demonstrates both the newly defined loss's ability to recover the probabilities of the underlying sample via pairwise relative samples and the ability if the model as a whole to provide post hoc adjustment according to the desired balance of accuracy and individual fairness.  The second example uses real world criminal recidivism data augmented with arbiter rating data to demonstrate the model can be performant in real world use.

\section{Background}

    \subsection{Bayesian Inference} \label{sec:intro_bayes}
    
        Bayesian statistics view the parameters of a statistical model as themselves being random variables based on an interpretation of probability as the uncertainty of belief about a system or outcome.  Mathematically, Bayesian statistics relies on Bayes' formula, derived from conditional probability as
        $$ \Pr(\theta | X) = \frac{\Pr(X | \theta) \Pr(\theta)}{\Pr(X)}. $$
        While classical (or frequentist) statistics focus exclusively on the \emph{likelihood} of the data ($\Pr(X | \theta)$ and what values of $\theta$ produce high likelihoods of the observed data, Bayesian statistics utlizes a \emph{posterior} (\emph{a posteriori}) distribution ($\Pr(\theta | X)$) of belief about $\theta$.  This posterior distribution is estimated via a \emph{prior} (\emph{a posteriori}) distribution ($\Pr(\theta)$, usually of belief about the distribution) of $\theta$ and, when a proportionate distribution is not sufficient, the \emph{evidence} ($Pr(X)$) of the data under the model.
        
        A variety of techniques have been developed for inference about parameters in Bayesian settings utilizing analytical, optimization, stochastic, and other approaches.  Chapter \ref{ch:TopicBlockModel} will utilize a stochastic method based on \emph{Monte Carlo sampling}.  Monte Carlo methods produce estimates about the posterior of a target parameter by producing samples from a distribution which approximates the posterior.  For high dimensional $\theta$, it is often intractible to produce samples of all parameters simultaneously, so sampling is accomplished by partitioning the parameter space and iteratively updating the partitions, e.g.
        $$ \Pr(\theta_1 | X, \theta_2) = \frac{\Pr(X | \theta_1, \theta_2) \Pr(\theta_1, \theta_2)}{\Pr(X, \theta_2)}. $$
        Since $\Pr(X, \theta_2)$ doesn't depend on $\theta_1$, it can be viewed as a normalizing constant in the posterior and is usually omitted, leading to a proportional expression,
        $$ \Pr(\theta_1 | X, \theta_2) \propto \Pr(X | \theta_1, \theta_2) \Pr(\theta_1, \theta_2).$$
        Depending on the exact setting, a variety of tools can be used to generate samples of $\theta_1$, including relatively old methods like the Metropolis\cite{metropolis1953equation} algorithm or Gibbs sampling\cite{geman1984stochastic} and relatively modern methods like Hamiltonian Monte Carlo\cite{girolami2011riemann} and the No-U-Turn Sampler (NUTS)\cite{hoffman2014no}.

    \subsection{Topic Models}  \label{sec:intro_topics}
    
        Topic models are a popular family of hierarchical Bayesian models for semantic analysis of corpora of documents.  The canonical model of this type is Latent Dirichlet Allocation \cite{blei2003LDA}, where each document is associated with a Dirichlet-distributed distribution over $T$ ``topics'', which themselves are Dirichlet-distributed distributions over words that tend to concentrate on semantically coherent topics. Each word  in the  document is assumed to have been generated by sampling a topic from that document's distribution over topics, and then sampling a word from the topic's distribution over words. 
        
        This basic model has been extended in a number of directions. A hierarchy of Dirichlet processes can be used to construct a Bayesian nonparametric variant with an unbounded number of topics \citep{Teh:2007}; a logistic normal distribution can be used to induce correlations between topics \citep{ctm}; time dependence has been incorporated to track topic evolution over time \citep{blei2006dynamic}.
    
    \subsection{Stochastic Blockmodels} \label{sec:intro_sbm}
    
    
        Stochastic blockmodels \citep{Wang:Wong:1987,Snijders:Nowicki:1997} are a popular class of generative models that assume that each node within a network is associated with one of $K$ latent clusters or communities. Each pair $(k,\ell)$ of communities is associated with a latent parameter $\lambda_{k,\ell}$, which parametrizes the interactions between members of those communities. Typically, the network is assumed to be binary, and the interactions are modeled as Bernoulli random variables.
        
        A number of variants to the basic stochastic blockmodel have been proposed. \citep{Karrer:Newman:2011} uses a gamma/Poisson link in place of a beta/Bernoulli, to obtain distributions over integer-valued networks, and also incorporates a per-node parameter that allows nodes in the same community to have different degree distribution. The Infinite Relational Model \citep{irm} allows a potentially infinite number of communities, with membership probabilities distributed according to a Dirichlet process. Rather than restrict each node to a single cluster, the Mixed Membership Stochastic Blockmodel \citep{Airoldi:Blei:Fienberg:Xing:2008} associates each node with a distribution over clusters, allowing nodes to perform several social roles.
    
    \subsection{Neural Networks}  \label{sec:intro_nns}
    
        Neural networks are a class of extremely functions defined by a series of alternating linear and non-linear transformations.  The theory that neural networks can act as universal function approximators goes back several decades\cite{cybenko1989approximation}, and since then their use has grown steadily.
        
        A single layer of a network can be expressed as
        $$ h_l = f_l( h_{l-1}; W_l, b_l, \sigma) = \sigma\left( W_l h_{l-1} + b_l \right) $$
        with $h_{l}$ being the output of the $l$'th layer, $W_l$ being a matrix of weights describing a linear transformation from $\mathbb{R}^{|h_{l-1}|}$ to $\mathbb{R}^{|h_l|}$, a bias vector $b_l$, and a non-linear transformation function on $\mathbb{R}^{|h_{l-1}|}$.  It commonly notated that $h_0$ is the input $x$ and $h_L$, i.e. the output of the final $L$'th layer, is the output $y$.
        
        There are several significant drawbacks of neural networks: the weight matrices $W_l$ incur an extremely large number of parameters, the combination of exchangeable weight matrices and non-linear activation function creates a highly-multimodal and non-convex parameter space, and the resulting functions are prone to overfitting sample data.
    
    \subsection{Fairness in Machine Learning}  \label{sec:intro_fairml}
    
        Machine learning algorithms trained to infer relationships, classify individuals or predict individuals' future performance tend to replicate biases inherent in the data \citep{Caliskan:Bryson:Narayanan:2017,Bornstein:2018,Angwin:Larson:Muttu:Kirchner:2016}. Worse, when these algorithms are used as tools in policy decision making, they can form parts of feedback loops that magnify discriminatory effects. For example, predictive policing algorithms aim to predict where crimes will take place, but are trained on data from where crimes are reported or arrests are made -- which can be skewed by biased policing and might not reflect the true crime map. If police officers are sent to areas with high predictive crime rate, they will tend to make more arrests there, increasing the algorithm's confidence and amplifying discrepancies between the crime rate and the arrest rate \citep{Ensign:2018,Lum:Isaac:2016}. 

    \subsubsection{Fairness Metrics}  \label{sec:intro_fairmetrics}
    
        Definitions of fairness in machine learning are generally (but not exclusively) divided into two camps based on their level of attention: group-level fairness and individual-level fairness.
        
        \emph{Individual fairness} aims to ensure that two individuals $u$ and $v$ with non-protected attributes $X_u, X_v$ have similar outcomes if $X_u$ and $X_v$ are similar, even if their protected attributes differ. Concretely, \citep{dwork2012fairness} describes a score function $f$ as individually fair if it is Lipschitz-continuous w.r.t.\ some metric $\mathcal{D}$ on $X$, i.e.\
        \begin{equation}d(f(X_u), f(X_v)) \le \mathcal{D}(X_u, X_v) ~\forall~ u, v \in \mathcal{U}\label{eqn:IndFair}\end{equation}
        where $d$ is a metric on the space of outcomes. This encapsulates the notion that if two individuals are similar in terms of non-protected attributes, they should have similar outcomes.
        
        Conversely, \emph{group fairness} metrics aim to minimize population-level imbalances. For example, the notion of demographic parity \citep{dwork2012fairness} requires that the predicted outcome $\hat{Y}$ is independent of the protected variable $A$. Equalized odds \citep{HarPriSre2016} requires that the predicted outcome $\hat{Y}$ is independent of $A$ conditioned on the true outcome $Y$, allowing a predictor to depend on $A$ via $Y$. Equalized opportunity \citep{HarPriSre2016} relaxes this condition in a classification task where the outcome $\hat{Y}=1$ is seen as more desirable than $\hat{Y}=0$, to require conditional independence between predictor $\hat{Y}$ and protected variable $\hat{A}$ only when $Y=1$. \cite{AgaBeyDudLanWal2018} show that demographic parity, equalized odds, and their variants can be expressed in terms of a set of linear constraints.  In many cases, individual notions of fairness are at odds with group notions of fairness. For example, \cite{dwork2012fairness} shows that individually fair functions achieve perfect demographic parity if and only if the distribution over individuals is similar across demographic groups.

    \subsection{Fair Methods}
        
        Since \citet{dwork2012fairness}, a variety of fairness-related methods have been developed, with the majority focusing on group fairness metrics. Some work to create fair representations \citep{dwork2012fairness,ZemQiSwePitDwo2013,MadCrePitZem2018} of the data so that whatever processes then use the representations can guarantee to be equally as fair.  These representations rarely consider individual fairness, although regularization techniques often produce an embedding function that is sufficiently smooth that a Lipschitz bound on the transformation could likely be made to estimate the distance function under which the representation has individual fairness.
        
        An alternative approach is to learn a single classifier on $X$ to predict $Y$, and to encourage fairness by regularization using a fairness-promoting penalty \citep{KamAkaSak2011,KamAkHidSak2012,berk2017convex} or constraints \citep{zafar2017parity,zafar2017aistats,AgaBeyDudLanWal2018}. If the classifiers used are Lipschitz-continuous, then they are all individually fair, since each individual is subject to the same classification function. The form of this function is governed by a trade-off between predictive accuracy, and some appropriate measure of (group-level) fairness. While this trade-off means regularization approaches may achieve lower accuracy and/or group-level fairness than representation-based approaches, their individual fairness yields transparency in implementation and avoids situations where individuals would have different outcomes under counterfactual protected attributes.

        Others have considered the idea of individual-level comparisons; \citet{balcan2018} explore the concept of ``envy freeness'' in classification in the context of individual-specific utility functions, where a classifier can be optimal when no individual's utility function would be higher if they received the predicted outcome (or distribution of outcomes) given to an individual with different attributes. \citet{lipton2018treatmentdisparity} study concepts of impact disparity and treatment disparity, where impact disparity is similar to statistical parity, that protected classes should be treated similarly overall.  They conceive of treatment disparity, a concept of individual fairness requiring that an individual's prediction not change if their protected attributes were counterfactually changed.